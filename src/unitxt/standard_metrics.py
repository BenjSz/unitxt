from abc import abstractmethod
from collections import Counter, defaultdict
from math import isclose, sqrt
from typing import Any, Dict, Generator, List, Optional

import numpy as np
from scipy.stats import poisson

from .operator import MultiStream, SingleStreamOperator, Stream, StreamInstanceOperator
from .type_utils import isoftype


class StandardGlobalMetric(SingleStreamOperator):
    calc_confidence_intervals: bool = True
    n_resamples: Optional[int] = None
    metric_name: str
    short_name: Optional[str] = None

    def disable_confidence_interval_calculation(self):
        self.calc_confidence_intervals = False

    @abstractmethod
    def single_instance_score(self, references: List[Any], prediction: Any):
        pass

    @abstractmethod
    def accumulate_instance_value(self, references: List[Any], prediction: Any):
        pass

    @abstractmethod
    def compute_final_from_aggregated(self) -> dict:
        pass

    def prepare(self):
        super().prepare()
        if not self.short_name:
            # e.g.  metrics.f1 -> f1, to show in the results. for rouge -> rougeL needs specific parameter
            self.short_name = self.metric_name[1 + self.metric_name.find(".") :]

    # flake8: noqa: C901
    def process(self, stream: Stream, stream_name: Optional[str] = None) -> Generator:
        from src import unitxt

        # def __add__(self, other):
        #     return type(self)(self.x + other.x, self.y + other.y)
        # clone = type(self.aggregator)(self.aggregator.metric_name)
        # https://stackoverflow.com/questions/2226330/instantiate-a-python-class-from-a-name
        # https://stackoverflow.com/questions/5924879/how-to-create-a-new-instance-from-a-class-object-in-python
        # https://stackoverflow.com/questions/14112179/using-class-to-create-instances

        # all the aggregators needed to evaluate ci over the metric for this stream
        # each of these will be updated once for each instance, and produce the final result at the end.
        # The instances do not stay in main memory. Only one instance at a time.
        ci_aggregators = []
        if self.calc_confidence_intervals:
            # original_standard_metric, _ = unitxt.artifact.fetch_artifact(self.metric_name)
            if self.n_resamples is None:
                self.n_resamples = unitxt.settings.num_resamples_for_global_metrics
            for _ in range(self.n_resamples):
                ci_aggregators.append(
                    type(self)()
                    # type(self.aggregator)(self.metric_name)
                )

        ###
        # confidence interval is computed by resampling. But, rather than having the whole stream laid in main memory,
        # and then generate the resamples from it, having them laid in memory next to it, and them measure over them,
        # We will randomly determine,
        # for the current instance i, and any of the planned resamples r, the number of times n, that i is to be
        # included in r.  What is the distribution of that number?
        # By the traditional resamples, over a set of s instances making the whole stream, a resample r is generated by
        # repeating the following process for s times: select an instance at random, all s instances having equal
        # probability 1/s to be selected, note the selected instance as a member of r, and return it to the original set
        # of s instances, so that in the next step, it maintain the same probability, 1/s to be selected.
        # All in all, the number of times b that a given instance i is selected to be included in a given resample r, is
        # binomially distributed with p = 1/s  and n = s.
        # For large n and small p (as is our case), the binomial distributions gets close to the poisson distribution with
        # mu = pn = 1 in our case.
        # So, given an instance i, for each resample r, we select at random, poissonically distributed, the number of
        # times b, that the instance is included in r,  and update the aggregators set for that resample, each by b
        # times.  For the lage number s of instances in the streams, the above process ensures, with high probability,
        # that the total number of times that of each of the per-resample aggregators is updated, is very close to
        # the length s of the stream. Which is the desired size of each resample, by the bootstrapping method.
        #

        total_num_of_instances = 0
        for instance in stream:
            total_num_of_instances += 1

            self.accumulate_instance_value(
                references=instance["references"], prediction=instance["prediction"]
            )

            if not self.calc_confidence_intervals:
                continue
            num_of_participations_of_this_instance_in_each_resample = poisson.rvs(
                mu=1, size=self.n_resamples
            )
            for i, aggregator in enumerate(ci_aggregators):
                for _ in range(
                    num_of_participations_of_this_instance_in_each_resample[i]
                ):
                    aggregator.accumulate_instance_value(
                        references=instance["references"],
                        prediction=instance["prediction"],
                    )

        if total_num_of_instances == 0:
            yield from stream

        ## compute all global scores from what was aggregated above in one pass over the stream

        global_score = self.compute_final_from_aggregated()
        global_score.update(
            {
                "score_name": self.short_name,  # for rouge make sure to change here to rougeL
                "score": global_score[self.short_name],
            }
        )

        # and the ci for the above score:
        if self.calc_confidence_intervals:
            bootstrap_scores = [
                ci_aggregator.compute_final_from_aggregated()[self.short_name]
                for ci_aggregator in ci_aggregators
            ]
            bootstrap_scores = [
                bootstrap_score
                for bootstrap_score in bootstrap_scores
                if bootstrap_score is not None and not np.isnan(bootstrap_score)
            ]
            ci = np.percentile(bootstrap_scores, [2.5, 97.5])
            global_score.update({self.short_name + "_ci_low": round(ci[0], 2)})
            global_score.update({self.short_name + "_ci_high": round(ci[1], 2)})
            # for backward compatibility
            score_short_name = global_score["score_name"]
            if score_short_name + "_ci_high" in global_score:
                global_score.update(
                    {"score_ci_high": global_score[score_short_name + "_ci_high"]}
                )
            if score_short_name + "_ci_low" in global_score:
                global_score.update(
                    {"score_ci_low": global_score[score_short_name + "_ci_low"]}
                )

            ## later stages demand instance["score"]["global"]["score"] for some averaging, which matches the score of "score_name"
            ## currently in branch 'main', these hold the metrics that happens to be computed last. needs to be looked into
            ## so for now, we did the same above

        ## update each instance of the whole stream with the news, and store its instance score

        class ScoreEachInstanceAndAddGlobalScore(StreamInstanceOperator):
            scorer: StandardGlobalMetric
            global_score: dict
            short_name: str

            def process(
                self, inst: Dict[str, Any], stream_name: Optional[str] = None
            ) -> Dict[str, Any]:
                if "score" not in inst:
                    inst["score"] = {"global": {}, "instance": {}}
                if "instance" not in inst["score"]:
                    inst["score"]["instance"] = {}
                if "global" not in inst["score"]:
                    inst["score"]["global"] = {}

                inst["score"]["global"].update(self.global_score)

                # compute instance score, and update it into the instance
                raw_instance_score = self.scorer.single_instance_score(
                    inst["references"], inst["prediction"]
                )
                inst["score"]["instance"].update(raw_instance_score)

                # for backward compatibility:
                inst["score"]["instance"].update(
                    {
                        "score_name": self.short_name,
                        "score": raw_instance_score[self.short_name],
                    }
                )

                return inst

        ms = MultiStream({"tmp": stream})
        score_each_instance_and_add_global_score = ScoreEachInstanceAndAddGlobalScore(
            scorer=self, global_score=global_score, short_name=self.short_name
        )
        ms = score_each_instance_and_add_global_score(ms)

        yield from ms["tmp"]


class ConfusionMatrixAggregator(StandardGlobalMetric):
    def prepare(self):
        super().prepare()
        self.confusion_matrix = Counter()
        self.num_of_instances_seen_thus_far = 0

    def single_instance_score(self, references: List[Any], prediction: Any) -> dict:
        # from F1(GlobalMetric) in metricts.py
        # assert (
        #     len(references) == 1
        # ), f"Only a single reference per prediction is allowed for {self.metric_name}"
        return {self.short_name: 1.0 if prediction in references else 0.0}

    def accumulate_instance_value(self, references: List[Any], prediction: Any):
        # from F1(GlobalMetric) in metricts.py
        # assert (
        #     len(references) == 1
        # ), f"Only a single reference per prediction is allowed for {self.metric_name}"
        self.confusion_matrix.update([(references[0], prediction)])
        self.num_of_instances_seen_thus_far += 1

    def compute_final_from_aggregated(self) -> dict:
        pass


class StandardAccuracy(ConfusionMatrixAggregator):
    metric_name = "standard_metrics.accuracy"

    def compute_final_from_aggregated(self) -> dict:
        # compute_acc_f1_micro_from_confusion_matrix()
        # e.g. from here: https://www.baeldung.com/cs/multi-class-f1-score
        # or from here: https://iamirmasoud.com/2022/06/19/understanding-micro-macro-and-weighted-averages-for-scikit-learn-metrics-in-multi-class-classification-with-example/
        if (
            self.num_of_instances_seen_thus_far == 0
        ):  # can happen with our tricky resampling, for very short streams
            return {self.short_name: np.nan}
        overall_true_positive = sum(
            self.confusion_matrix[r, p]
            for r, p in self.confusion_matrix.keys()
            if r == p
        )
        precision = float(overall_true_positive) / self.num_of_instances_seen_thus_far
        # for micro, overall_precision == overall_recall, we thus have:
        # our_micro_f1 = 2 * precision * precision / (precision + precision) = precision
        # and unlike the multi_level, we have no true negative here, hence
        # also accuracy == precision
        # as nicely explained here:
        # https://simonhessner.de/why-are-precision-recall-and-f1-score-equal-when-using-micro-averaging-in-a-multi-class-problem/
        return {self.short_name: round(precision, 2)}


class StandardF1Micro(StandardAccuracy):
    metric_name = "metrics.f1_micro"
    # return the same result, just a different name
    pass


class StandardF1Macro(ConfusionMatrixAggregator):
    metric_name = "standard_metrics.f1_macro"

    def compute_final_from_aggregated(self) -> dict:
        # def compute_f1_macro_from_confusion_matrix(self) -> Any:
        # e.g. from here: https://www.baeldung.com/cs/multi-class-f1-score
        # or from here: https://iamirmasoud.com/2022/06/19/understanding-micro-macro-and-weighted-averages-for-scikit-learn-metrics-in-multi-class-classification-with-example/
        if (
            self.num_of_instances_seen_thus_far == 0
        ):  # can happen with our tricky resampling, for very short streams
            return {self.short_name: np.nan}
        true_classes = list(
            {r for r, p in self.confusion_matrix.keys()}
        )  # true classes
        predictions = list(
            {p for r, p in self.confusion_matrix.keys()}
        )  # predictions, could also be a super set or a subset of classes
        intersect = [r for r in true_classes if r in predictions]
        precision = {
            pred: float(self.confusion_matrix[(pred, pred)])
            / float(
                sum(
                    self.confusion_matrix[(r, p)]
                    for r, p in self.confusion_matrix.keys()
                    if p == pred
                )
            )
            for pred in intersect
        }
        recall = {
            ref: float(self.confusion_matrix[(ref, ref)])
            / float(
                sum(
                    self.confusion_matrix[(r, p)]
                    for r, p in self.confusion_matrix.keys()
                    if r == ref
                )
            )
            for ref in intersect
        }

        f1 = {
            "f1_" + str(c): round(
                2 * precision[c] * recall[c] / (precision[c] + recall[c]), 2
            )
            if (precision[c] + recall[c]) > 0
            else 0.0
            for c in intersect
        }
        # for classes that never showed in any prediction, we have recall = 0,
        # and for classes that only showed as predictions (string-perturbated of class name) we have precision == 0
        # at any rate, these deserve f1 = 0, to contribute to average..
        f1.update({"f1_" + str(c): 0.0 for c in predictions if c not in intersect})
        f1.update({"f1_" + str(c): 0.0 for c in true_classes if c not in intersect})
        our_f1_macro = sum(f1.values()) / float(
            len(f1)
        )  # un weighted average over the classes

        f1.update({"f1_macro": round(our_f1_macro, 2)})
        return f1


class StandardMatthewsCorrelation(ConfusionMatrixAggregator):
    metric_name = "standard_metrics.matthews_correlation"

    def single_instance_score(self, references: List[Any], prediction: Any) -> dict:
        # matthews along a list of one pair of items is 0 by definition
        return {self.short_name: 0.0}

    def compute_final_from_aggregated(self) -> dict:
        # def compute_matthews_correlation_coefficient_from_confusion_matrix() -> Any:
        # follow https://dwbi1.wordpress.com/2022/10/05/mcc-formula-for-multiclass-classification/   that follows scikit-learn
        classes = list({r for r, p in self.confusion_matrix.keys()})  # true classes
        predictions = list(
            {p for r, p in self.confusion_matrix.keys()}
        )  # predictions, could also be a super set or a subset of classes
        intersect = [r for r in classes if r in predictions]
        tk = {
            ref: sum(
                self.confusion_matrix[(r, p)]
                for r, p in self.confusion_matrix.keys()
                if r == ref
            )
            for ref in classes
        }
        tksquared = sum(tk[ref] * tk[ref] for ref in tk.keys())
        pk = {
            pred: sum(
                self.confusion_matrix[(r, p)]
                for r, p in self.confusion_matrix.keys()
                if p == pred
            )
            for pred in predictions
        }
        pksquared = sum(pk[pred] * pk[pred] for pred in pk.keys())

        c = sum(self.confusion_matrix[(r, r)] for r in classes)
        s = sum(self.confusion_matrix.values())
        nominator = s * c - sum(tk[c] * pk[c] for c in intersect)
        denominator = sqrt(float(s * s - pksquared)) * sqrt(float(s * s - tksquared))

        if isclose(denominator, 0.0):
            mcc = 0.0
        else:
            mcc = nominator / denominator
        # self.logger.info(mcc)
        return {self.short_name: mcc}


class ConfusionMatrixForMultiLabelAggregator(StandardGlobalMetric):
    def prepare(self):
        super().prepare()
        self.classes_seen_thus_far = set()
        self.references_seen_thus_far = set()
        self.tp = defaultdict(int)  # true/false positive/negative of what seen thus far
        self.tn = defaultdict(int)
        self.fp = defaultdict(int)
        self.fn = defaultdict(int)
        self.num_of_instances_seen_thus_far = 0

    # adapted from metrics.py for multilabel
    def _validate_references_and_prediction(
        self, references: List[Any], prediction: Any
    ):
        if not len(references) == 1:
            raise ValueError(
                f"Only a single reference per instance is allowed in multi label metric. Received reference: {references}"
            )
        if not isoftype(references[0], List[str]):
            raise ValueError(
                f"Instance references is expected to be a list of one item being a list of strings in multi label metric. Received references: '{references}'"
            )

        if not isoftype(prediction, List[str]):
            raise ValueError(
                f"Instance prediction is expected to be a list of strings in multi label metric. Received prediction: '{prediction}'"
            )
        if not len(set(prediction)) == len(prediction):
            raise ValueError(
                f"Elements of prediction are expected to be distinct strings, in multi label metric. Received prediction: '{prediction}'"
            )
        if not len(set(references[0])) == len(references[0]):
            raise ValueError(
                f"Elements of references[0] are expected to be distinct strings, in multi label metric. Received references[0]: '{references[0]}'"
            )

    def single_instance_score(self, references: List[Any], prediction: Any) -> float:
        self._validate_references_and_prediction(
            references=references, prediction=prediction
        )
        num_of_hits = len([pred for pred in prediction if pred in references[0]])
        # this method is invoked for storing instance[score][instance], after global was computed,
        # so self.classes_seen_thus_far is updated with all classes ever seen in pred or ref
        # for coherence, and in order to clean results, we only report on classes ever seen in references
        # num_of_true_misses = len(
        #     [
        #         ref
        #         for ref in self.references_seen_thus_far
        #         if ref not in prediction and ref not in references[0]
        #     ]
        # )
        # hit_ratio = float(num_of_hits + num_of_true_misses) / len(prediction)
        hit_ratio = 0.0 if len(prediction) == 0 else num_of_hits / len(prediction)
        return {self.short_name: round(hit_ratio, 2)}

    def accumulate_instance_value(self, references: List[Any], prediction: Any):
        self._validate_references_and_prediction(
            references=references, prediction=prediction
        )
        # two lists of distinct str (distinct inside each list), reference is wrapped.
        # we increase tp for each member of pred that is also in ref.
        # we increase fp for each member of pred that is not in ref,
        # we increase fn for each member of ref that is not in pred.
        # we increase tn for all members of classes that we know of, that are missing from both,
        #
        # once a new class becomes known to us, we increase its tn by the number of instances seen thus
        # far (not including this one).
        for pred in prediction:
            if pred in references[0]:
                self.tp[pred] += 1
            else:
                self.fp[pred] += 1
        for ref in references[0]:
            self.references_seen_thus_far.add(ref)
            if ref not in prediction:
                self.fn[ref] += 1
        for c in self.classes_seen_thus_far:
            if c not in prediction and c not in references[0]:
                self.tn[c] += 1
        for d in prediction + references[0]:
            if d not in self.classes_seen_thus_far:
                self.tn[d] = self.num_of_instances_seen_thus_far
                self.classes_seen_thus_far.add(d)
        self.num_of_instances_seen_thus_far += 1

    def compute_final_from_aggregated(self) -> dict:
        pass


class StandardAccuracyMultiLabel(ConfusionMatrixForMultiLabelAggregator):
    metric_name = "standard_metrics.accuracy_multi_label"

    def compute_final_from_aggregated(self) -> dict:
        # def compute_accuracy_and_f1_micro_multi_label_from_confusion_matrix(self):
        if self.num_of_instances_seen_thus_far == 0:
            return {self.short_name: np.nan}
        total_tp = sum(v for k, v in self.tp.items())
        total_tn = sum(v for k, v in self.tn.items())
        total_fp = sum(v for k, v in self.fp.items())
        total_fn = sum(v for k, v in self.fn.items())
        acc = (total_tp + total_tn) / (total_tp + total_tn + total_fp + total_fn)
        return {self.short_name: round(acc, 2)}


class StandardF1MicroMultiLabel(ConfusionMatrixForMultiLabelAggregator):
    metric_name = "standard_metric.f1_micro_multi_label"

    def compute_final_from_aggregated(self) -> dict:
        # def compute_accuracy_and_f1_micro_multi_label_from_confusion_matrix(self):
        # https://medium.com/synthesio-engineering/precision-accuracy-and-f1-score-for-multi-label-classification-34ac6bdfb404
        if self.num_of_instances_seen_thus_far == 0:
            return {self.short_name: np.nan}
        total_tp = sum(v for k, v in self.tp.items())
        if total_tp == 0:
            return {self.short_name: 0.0}
        total_fp = sum(v for k, v in self.fp.items())
        total_fn = sum(v for k, v in self.fn.items())

        total_recall = total_tp / (total_fn + total_tp)
        total_precision = total_tp / (total_tp + total_fp)
        f1_micro = 2 * total_recall * total_precision / (total_recall + total_precision)
        return {"f1_micro_multi_label": round(f1_micro, 2)}


class StandardF1MacroMultiLabel(ConfusionMatrixForMultiLabelAggregator):
    metric_name = "standard_metric.f1_macro_multi_label"

    def compute_final_from_aggregated(self) -> dict:
        # def compute_f1_macro_multi_label_from_confusion_matrix(self) -> Any:
        # e.g. from here: https://medium.com/synthesio-engineering/precision-accuracy-and-f1-score-for-multi-label-classification-34ac6bdfb404
        # report only for the classes seen as references
        if len(self.references_seen_thus_far) == 0:
            return {self.short_name: np.nan}
        to_ret = {}
        for c in self.references_seen_thus_far:  # report only on them
            num_as_pred = self.tp[c] + self.fp[c]
            precision = 0.0 if num_as_pred == 0 else self.tp[c] / num_as_pred
            num_as_ref = self.tp[c] + self.fn[c]
            recall = np.nan if num_as_ref == 0 else self.tp[c] / num_as_ref
            f1 = (
                0.0
                if np.isnan(precision)
                or np.isnan(recall)
                or isclose(precision, 0)
                or isclose(recall, 0)
                else 2 * precision * recall / (precision + recall)
            )
            to_ret["f1_" + c] = round(f1, 2)
        avg_across_classes = (
            sum(val for val in to_ret.values() if not np.isnan(val))
        ) / len(self.references_seen_thus_far)
        to_ret[self.short_name] = round(avg_across_classes, 2)
        return to_ret
