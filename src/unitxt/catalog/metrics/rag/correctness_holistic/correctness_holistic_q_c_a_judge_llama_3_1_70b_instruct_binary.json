{
    "__type__": "metric_pipeline",
    "main_score": "correctness_holistic_q_c_a_judge_llama_3_1_70b_instruct_binary",
    "metric": {
        "__type__": "generative_binary_judge_wml",
        "main_score": "correctness_holistic_q_c_a_judge_llama_3_1_70b_instruct_binary",
        "model_name": "meta-llama/llama-3-1-70b-instruct",
        "task_name": "tasks.rag_eval.correctness_holistic.binary",
        "template_name": "templates.rag_eval.correctness_holistic.judge_correctness_simple_logprobs",
        "model_format_name": "formats.llama3_instruct",
        "binary_predictions": true
    },
    "preprocess_steps": [
        {
            "__type__": "copy",
            "field_to_field": {
                "prediction": "task_data/answer",
                "task_data/reference_answers": "task_data/ground_truths"
            },
            "not_exist_ok": true
        },
        {
            "__type__": "copy",
            "field_to_field": {
                "data_classification_policy": "task_data/data_classification_policy"
            },
            "not_exist_ok": true,
            "get_default": [
                "public"
            ]
        },
        {
            "__type__": "set",
            "fields": {
                "prediction": 0.0,
                "references": [
                    0.0
                ]
            }
        }
    ]
}
