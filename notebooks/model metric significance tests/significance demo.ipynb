{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1abdae75",
   "metadata": {},
   "source": [
    "Demo of visualizations of significance testing of dataset metrics between different LMMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6933dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "newpath = os.path.join(os.getcwd(), \"..\", \"..\", \"src\")\n",
    "sys.path.append(os.path.realpath(newpath))\n",
    "\n",
    "from unitxt.metric_paired_significance import PairedDifferenceTest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba06c4",
   "metadata": {},
   "source": [
    "Assume we have a dataset of records (e.g., questions with ground truth answers, and predictions from several different models we want to compare).  For each record, we can calculate a metric value of the model accuracy (e.g., recall, precision, s-bert similarity).  Given a metric, we can compare model A vs B by testing whether, statistically, the metric value is very different (or higher/lower) on predictions from model A or B.  The metric values are *paired* (since the underlying record is the same, and the records are not repeated random samples) so the tests conducted assume paired observations.\n",
    "\n",
    "Significance is defined in two ways:\n",
    "- The p-value, which takes values in [0,1], and lower values indicate a higher significant difference\n",
    "- The effect size, which overcomes the p-value tendency to overstate differences when the sample size is large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f8dd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data\n",
    "data = {mm: pd.read_csv('{}.csv'.format(mm)) for mm in [\"recall\", \"precision\", \"sbert\"]}\n",
    "# each such dataset has the same columns\n",
    "print(data['recall'].head())\n",
    "# test combinations vs each other\n",
    "tester_full = PairedDifferenceTest(nmodels=6, model_names=data[\"recall\"].columns)\n",
    "# format dataframe as a list of sample tuples compabible with the tester\n",
    "samples = {kk: tester_full.format_as_samples(samples_list=[vv[cc].to_numpy() for cc in vv.columns], metric=kk)\n",
    "          for kk, vv in data.items()}\n",
    "\n",
    "\n",
    "# perform test on each metric separately\n",
    "# default two-sided test (is metric value significantly DIFFERENT)\n",
    "test_results_twoside = [tester_full.signif_pair_diff(samples_list=vv) for vv in samples.values()]\n",
    "# perform one-sided test (is metric in first model significantly lower than in the second)\n",
    "test_results_leftside = [tester_full.signif_pair_diff(samples_list=vv, alternative='less') for vv in samples.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e7033",
   "metadata": {},
   "source": [
    "Display the resulting objects containing the test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ecf70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"two-sided (metrics are different)\")\n",
    "print(test_results_twoside[0])\n",
    "# when changing the alternative hypothesis, only the p-values and their significances, and the significance of the effect size\n",
    "print(\"left-sided (model A < model B)\")\n",
    "print(test_results_leftside[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0300c3",
   "metadata": {},
   "source": [
    "A heatmap takes the tests between models across multiple metrics.  It shows which model pairs appear significantly different across the metrics.  More significant comparisons are displayed higher in the heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d88c2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# heatmap comparing multiple metrics\n",
    "# optimize_color=False keeps the original order of comparison of models in each pair\n",
    "# color optimization flips the order of comparison if it improves color uniformity\n",
    "tester_full.multiple_metrics_significance_heatmap(test_results_twoside, optimize_color=False)\n",
    "print(\"color optimized (default)\")\n",
    "tester_full.multiple_metrics_significance_heatmap(test_results_twoside)\n",
    "\n",
    "tester_full.multiple_metrics_significance_heatmap(test_results_leftside)\n",
    "# use Cohen's d effect size instead\n",
    "tester_full.multiple_metrics_significance_heatmap(test_results_twoside, use_pvalues=False, optimize_color=False)\n",
    "print(\"color optimized (default)\")\n",
    "tester_full.multiple_metrics_significance_heatmap(test_results_twoside, use_pvalues=False)\n",
    "\n",
    "tester_full.multiple_metrics_significance_heatmap(test_results_leftside, use_pvalues=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75884dcb",
   "metadata": {},
   "source": [
    "Plot a connected graph, were nodes represent models.\n",
    "The vertical orientation of nodes corresponds to the mean value of the metric.\n",
    "Nodes that are significantly different are connected by an edge; thicker edges mean a less significant difference.  For example, if all comparisons are significant, we will see all nodes unconnected, without edges.  The graph allows us to visualize groupings of similarly-performing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42717459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use retrieval and model to code the node drawings with color\n",
    "# graph shows NOT SIGNIFICANT pairwise comparisons connected by an edge\n",
    "\n",
    "node_color_levels = [mn.split('_')[1] for mn in tester_full.model_names]\n",
    "print(tester_full.model_names)\n",
    "\n",
    "for res in test_results_twoside + test_results_leftside:\n",
    "    tester_full.metric_significant_pairs_graph(test_res=res, model_name_split_char=\"_\", node_color_levels=node_color_levels, weight_edges=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce8ca29",
   "metadata": {},
   "source": [
    "A lineplot shows p-values by connecting the compared models with a line segment.  Initially the models are arranged along the y-axis according to the sample mean of the metric; each model is assigned a given color.  The horizontal location of the line segment is at the p-value, with more significant comparisons placed to the left.  Using this method, it is not as easy to visualize the significant comparisons because the segments tend to be overplotted.  We recommend the connected graph above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555bb936",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lineplot shows p-values of pairwise comparisons by significance\n",
    "# significant pairs are shown toward the left side of the plot\n",
    "for vv in test_results_twoside:\n",
    "    tester_full.pvalue_lineplot(vv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unitxt",
   "language": "python",
   "name": "unitxt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
