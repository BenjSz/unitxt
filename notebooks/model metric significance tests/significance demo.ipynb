{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff661461",
   "metadata": {},
   "source": [
    "Demo of visualizations of significance testing of dataset metrics between different LMMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b132390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\",\"..\",\"src\"))\n",
    "from unitxt.metric_paired_significance import PairedDifferenceTest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491641d8",
   "metadata": {},
   "source": [
    "Assume we have a dataset of records (e.g., questions with ground truth answers, and predictions from several different models we want to compare).  For each record, we can calculate a metric value of the model accuracy (e.g., recall, precision, s-bert similarity).  Given a metric, we can compare model A vs B by testing whether, statistically, the metric value is very different (or higher/lower) on predictions from model A or B.  The metric values are *paired* (since the underlying record is the same, and the records are not repeated random samples) so the tests conducted assume paired observations.\n",
    "\n",
    "Significance is defined in two ways:\n",
    "- The p-value, which takes values in [0,1], and lower values indicate a higher significant difference\n",
    "- The effect size, which overcomes the p-value tendency to overstate differences when the sample size is large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5f8bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data\n",
    "data = {mm: pd.read_csv('{}.csv'.format(mm)) for mm in [\"recall\", \"precision\", \"sbert\"]}\n",
    "print(data['recall'].head())\n",
    "# test combinations vs each other\n",
    "tester_full = PairedDifferenceTest(nmodels=6, model_names=data[\"recall\"].columns)\n",
    "\n",
    "# perform test on each metric separately\n",
    "test_results_twoside = [tester_full.signif_pair_diff(samples_list=[vv[cc].to_numpy() for cc in vv.columns], metric_name=kk) for kk, vv in data.items()]\n",
    "# perform one-sided test (for example)\n",
    "test_results_leftside = [tester_full.signif_pair_diff(samples_list=[vv[cc].to_numpy() for cc in vv.columns], metric_name=kk, alternative='less') for kk, vv in data.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfeec34",
   "metadata": {},
   "source": [
    "Display the resulting objects containing the test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f20281",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"two-sided (metrics are different)\")\n",
    "print(test_results_twoside[0])\n",
    "print(\"left-sided (model A < model B)\")\n",
    "print(test_results_leftside[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284d12a1",
   "metadata": {},
   "source": [
    "A heatmap takes the tests between models across multiple metrics.  It shows which model pairs appear significantly different across the metrics.  More significant comparisons are displayed higher in the heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1dfaf0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# heatmap comparing multiple metrics\n",
    "tester_full.multiple_metrics_significance_heatmap(test_results_twoside, optimize_color=False)\n",
    "print(\"color optimized\")\n",
    "tester_full.multiple_metrics_significance_heatmap(test_results_twoside)\n",
    "\n",
    "tester_full.multiple_metrics_significance_heatmap(test_results_leftside)\n",
    "# use Cohen's d effect size instead\n",
    "tester_full.multiple_metrics_significance_heatmap(test_results_twoside, use_pvalues=False, optimize_color=False)\n",
    "print(\"color optimized\")\n",
    "tester_full.multiple_metrics_significance_heatmap(test_results_twoside, use_pvalues=False)\n",
    "\n",
    "tester_full.multiple_metrics_significance_heatmap(test_results_leftside, use_pvalues=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dc8fec",
   "metadata": {},
   "source": [
    "Plot a connected graph, were nodes represent models.\n",
    "The vertical orientation of nodes corresponds to the mean value of the metric.\n",
    "Nodes that are significantly different are connected by an edge; thicker edges mean a less significant difference.  For example, if all comparisons are significant, we will see all nodes unconnected, without edges.  The graph allows us to visualize groupings of similarly-performing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e891a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use retrieval and model to code the node drawings with color\n",
    "# graph shows NOT SIGNIFICANT pairwise comparisons connected by an edge\n",
    "\n",
    "node_color_levels = [mn.split('_')[1] for mn in tester_full.model_names]\n",
    "\n",
    "for res in test_results_twoside + test_results_leftside:\n",
    "    tester_full.metric_significant_pairs_graph(test_res=res, model_name_split_char=\"_\", node_color_levels=node_color_levels, weight_edges=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2f5ac8",
   "metadata": {},
   "source": [
    "A lineplot shows p-values by connecting the compared models with a line segment.  Initially the models are arranged along the y-axis according to the sample mean of the metric; each model is assigned a given color.  The horizontal location of the line segment is at the p-value, with more significant comparisons placed to the left.  Using this method, it is not as easy to visualize the significant comparisons because the segments tend to be overplotted.  We recommend the connected graph above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86fc499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lineplot shows p-values of pairwise comparisons by significance\n",
    "# significant pairs are shown toward the left side of the plot\n",
    "for vv in test_results_twoside:\n",
    "    tester_full.pvalue_lineplot(vv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unitxt",
   "language": "python",
   "name": "unitxt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
