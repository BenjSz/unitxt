owner,started_at,framework,benchmark,dataset,task,model_name,score,score_name,all_scores,run_params
ilab,49:15.4,Unitxt,ilab,rag.response_generation.clapnq,rag,models/merlinite-7b-lab-Q4_K_M.gguf,0.398435842,f1,"{'f1': np.float64(0.39843584240353835), 'score': np.float64(0.39843584240353835), 'precision': np.float64(0.5610636760338862), 'recall': np.float64(0.44515053954819467), 'precision_ci_low': np.float64(0.5041014283585713), 'precision_ci_high': np.float64(0.62063015015662), 'recall_ci_low': np.float64(0.3872341987836122), 'recall_ci_high': np.float64(0.5015637402298777), 'f1_ci_low': np.float64(0.3605488533498336), 'f1_ci_high': np.float64(0.43778275117639087), 'score_ci_low': np.float64(0.3605488533498336), 'score_ci_high': np.float64(0.43778275117639087), 'correctness_f1_bert_score.deberta_large_mnli': 0.6393274900317192, 'correctness_recall_bert_score.deberta_large_mnli': 0.6284224116802215, 'correctness_precision_bert_score.deberta_large_mnli': 0.6744405928254128, 'faithfullness_f1_token_overlap': np.float64(0.3186865068233722), 'faithfullness_recall_token_overlap': np.float64(0.24551629251584114), 'faithfullness_precision_token_overlap': np.float64(0.7842825674392097), 'correctness_f1_token_overlap': np.float64(0.39843584240353835), 'correctness_recall_token_overlap': np.float64(0.44515053954819467), 'correctness_precision_token_overlap': np.float64(0.5610636760338862)}","{'loader_limit': '100', 'host': 'cccxc443', 'folder': 'instructlab', 'num_shots': 0, 'template':'templates.rag.response_generation.please_respond'}"