owner,started_at,framework,benchmark,dataset,task,model_name,score,score_name,all_scores,run_params
ilab,2024-08-18 10:15:13.582791,Unitxt,ilab,rag.response_generation.clapnq,rag,models/merlinite-7b-lab-Q4_K_M.gguf,0.31122166034597654,f1,"{'f1': np.float64(0.31122166034597654), 'score': np.float64(0.31122166034597654), 'precision': np.float64(0.25834835149062685), 'recall': np.float64(0.732913827875705), 'precision_ci_low': np.float64(0.21695224173057687), 'precision_ci_high': np.float64(0.3098852529870915), 'recall_ci_low': np.float64(0.682684535651541), 'recall_ci_high': np.float64(0.7699121261724281), 'f1_ci_low': np.float64(0.27304408831302623), 'f1_ci_high': np.float64(0.35701370344468003), 'score_ci_low': np.float64(0.27304408831302623), 'score_ci_high': np.float64(0.35701370344468003), 'correctness_f1_bert_score.deberta_large_mnli': 0.5876904284954071, 'correctness_recall_bert_score.deberta_large_mnli': 0.7244832047820091, 'correctness_precision_bert_score.deberta_large_mnli': 0.5134033036231994, 'faithfullness_f1_token_overlap': np.float64(0.4130116436163791), 'faithfullness_recall_token_overlap': np.float64(0.5293569917788146), 'faithfullness_precision_token_overlap': np.float64(0.4840775590999496), 'correctness_f1_token_overlap': np.float64(0.31122166034597654), 'correctness_recall_token_overlap': np.float64(0.732913827875705), 'correctness_precision_token_overlap': np.float64(0.25834835149062685)}","{'loader_limit': '100', 'host': 'cccxc443', 'folder': 'instructlab', 'num_shots': 5}"
