owner,started_at,framework,benchmark,dataset,task,model_name,score,score_name,all_scores,run_params
ilab,2024-08-18 10:16:07.302044,Unitxt,ilab,rag.response_generation.clapnq,rag,models/merlinite-7b-lab-Q4_K_M.gguf,0.4030009241509693,f1,"{'llama_3_70b_instruct_ibm_genai_template_generic_single_turn': 0.74, 'score': np.float64(0.4030009241509693), 'llama_3_70b_instruct_ibm_genai_template_generic_single_turn_ci_low': np.float64(0.6422623797562254), 'llama_3_70b_instruct_ibm_genai_template_generic_single_turn_ci_high': np.float64(0.8), 'score_ci_low': np.float64(0.1822460037812891), 'score_ci_high': np.float64(0.6356879432624114), 'f1': np.float64(0.4030009241509693), 'precision': np.float64(0.6507959330389237), 'recall': np.float64(0.3806291944695498), 'precision_ci_low': np.float64(0.23204489852716179), 'precision_ci_high': np.float64(0.9282051282051281), 'recall_ci_low': np.float64(0.17035923141186296), 'recall_ci_high': np.float64(0.5335566110502172), 'f1_ci_low': np.float64(0.1822460037812891), 'f1_ci_high': np.float64(0.6356879432624114), 'correctness_f1_bert_score.deberta_large_mnli': 0.6427458763122559, 'correctness_recall_bert_score.deberta_large_mnli': 0.6260657668113708, 'correctness_precision_bert_score.deberta_large_mnli': 0.6954382061958313, 'faithfullness_f1_token_overlap': np.float64(0.2804252129078551), 'faithfullness_recall_token_overlap': np.float64(0.20955247347822584), 'faithfullness_precision_token_overlap': np.float64(0.8467768991133477), 'correctness_f1_token_overlap': np.float64(0.4030009241509693), 'correctness_recall_token_overlap': np.float64(0.3806291944695498), 'correctness_precision_token_overlap': np.float64(0.6507959330389237)}",{}
